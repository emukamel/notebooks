{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General approach\n",
    "\n",
    "Our goal is to construct a low-rank approximation of a data matrix $A$ of DNA methylation patterns (with many missing data points). Each row of $A$ contains a binary sequence of methylation, $A_{ij} = 1$ for methylated sites, $A_{ij} = 0$ for unmethylated sites, and $A_{ij} = \\text{NA}$ when site $j$ is unobserved in read $i$. We factor $A$ as a product of two matrices $X$ and $Y$:\n",
    "\n",
    "$$\n",
    "A = XY \\\\ A \\in \\mathbf{R}^{m \\times n},~~ X \\in \\mathbf{R}^{m \\times k},~~ Y \\in \\mathbf{R}^{k \\times n}\n",
    "$$\n",
    "\n",
    "That is, we have a dataset $m$ reads that provide partial information over $n$ methylation-eligible sites of interest (e.g. over a selected number of genes, or, more ambitiously, over the entire genome). We suppose that there are no more than $k$ types of cells with distinct methylation patterns.\n",
    "\n",
    "The rows of $Y$ provide the estimates for the $k$ distinct methylation patterns. A row of $X$ assigns the relative probabilities for that read to one of the $k$ patterns. An interesting question is whether we believe the rows of $X$ to be sparse. A sparse rowspace of $X$ would correspond to there being distinct methylation patterns across cell types, with little to no mixed patterns. A non-sparse rowspace would allow for a spectrum of different DNA methylation profiles.\n",
    "\n",
    "### Optimization Problems\n",
    "\n",
    "We use $x_i$ to denote row $i$ of $X$, and $y_j$ to denote column $j$ of $Y$, so that $x_i y_j$ is a vector dot product. Let $(i,j) \\in \\Omega$ denote the indices of observed entries in $A$. Then a low-rank approximation can be achieved by solving the unconstrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{X,Y}{\\text{minimize}}\n",
    "& & \\sum_{(i,j) \\in \\Omega} \\log(1+\\exp(-A_{ij} (x_i y_j)) - \\gamma ||X||_F^2 - \\gamma||Y||_F^2 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first term is a logistic loss function. The last two are regularization terms that penalize the *Frobenius norm* of the matrices. [Udell et al. (2014)](http://web.stanford.edu/~udell/doc/glrm.pdf) show that this problem is equivalent to the rank-constrained nuclear-norm regularized problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{X,Y}{\\text{minimize}}\n",
    "& & \\sum_{(i,j) \\in \\Omega} \\log(1+\\exp(-A_{ij} (x_i y_j)) - 2 \\gamma ||XY||_*^2 \\\\\n",
    "& \\text{subject to}\n",
    "& & \\text{rank}(Z) \\leq k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "as long as $k$ is chosen to be larger than the true rank of $A$. The *Frobenius norm* and *nuclear norm* are respectively defined as:\n",
    "\n",
    "$$\n",
    "||X||_F = \\sqrt{\\sum_{i=1}^k \\sigma_i^2} = \\sqrt{ \\text{trace} \\left ( X^T X \\right ) } \\\\\n",
    "||X||_* = \\sum_{i=1}^k \\sigma_i = \\text{trace} \\left ( \\sqrt{X^T X} \\right )\n",
    "$$\n",
    "\n",
    "### Relevant Papers on Matrix Completion\n",
    "\n",
    "Emmanuel J. Candès, Benjamin Recht. [Exact Matrix Completion via Convex Optimization](http://dx.doi.org/10.1007/s10208-009-9045-5). *Found Comput Math* 9, 717–772 (2009)\n",
    "\n",
    "Emmanuel J. Candès, Terence Tao. [The Power of Convex Relaxation: Near-Optimal Matrix Completion](http://dx.doi.org/10.1109/tit.2010.2044061). *IEEE Transactions on Information Theory* 56, 2053–2080 (2010).\n",
    "\n",
    "Emmanuel J Candès, Yaniv Plan. [Matrix Completion With Noise](http://dx.doi.org/10.1109/jproc.2009.2035722). *Proc. IEEE* 98, 925–936 (2010).\n",
    "\n",
    "Benjamin Recht, Maryam Fazel, Pablo A. Parrilo. [Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization](http://dx.doi.org/10.1137/070697835). *SIAM Rev.* 52, 471–501 (2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLRM(100x100 Array{Float64,2}:\n",
       " -0.0587522   0.804001  0.496302  …  -0.290197   -0.413114  -0.877346\n",
       " -0.798089    2.23288   1.83455       0.966752   -0.677168  -0.656077\n",
       "  1.39135    -0.737739  0.9028       -0.116393   -0.941991   1.70175 \n",
       "  0.550548    0.522705  0.907778     -0.485389   -1.35808   -2.5533  \n",
       "  0.442172    0.239211  0.805606     -0.0264728  -0.913191   1.00261 \n",
       " -0.0587522   0.804001  0.496302  …  -0.290197   -0.413114  -0.877346\n",
       " -0.798089    2.23288   1.83455       0.966752   -0.677168  -0.656077\n",
       "  1.39135    -0.737739  0.9028       -0.116393   -0.941991   1.70175 \n",
       "  0.550548    0.522705  0.907778     -0.485389   -1.35808   -2.5533  \n",
       "  0.442172    0.239211  0.805606     -0.0264728  -0.913191   1.00261 \n",
       " -0.0587522   0.804001  0.496302  …  -0.290197   -0.413114  -0.877346\n",
       " -0.798089    2.23288   1.83455       0.966752   -0.677168  -0.656077\n",
       "  1.39135    -0.737739  0.9028       -0.116393   -0.941991   1.70175 \n",
       "  ⋮                               ⋱                                  \n",
       "  0.550548    0.522705  0.907778     -0.485389   -1.35808   -2.5533  \n",
       "  0.442172    0.239211  0.805606     -0.0264728  -0.913191   1.00261 \n",
       " -0.0587522   0.804001  0.496302  …  -0.290197   -0.413114  -0.877346\n",
       " -0.798089    2.23288   1.83455       0.966752   -0.677168  -0.656077\n",
       "  1.39135    -0.737739  0.9028       -0.116393   -0.941991   1.70175 \n",
       "  0.550548    0.522705  0.907778     -0.485389   -1.35808   -2.5533  \n",
       "  0.442172    0.239211  0.805606     -0.0264728  -0.913191   1.00261 \n",
       " -0.0587522   0.804001  0.496302  …  -0.290197   -0.413114  -0.877346\n",
       " -0.798089    2.23288   1.83455       0.966752   -0.677168  -0.656077\n",
       "  1.39135    -0.737739  0.9028       -0.116393   -0.941991   1.70175 \n",
       "  0.550548    0.522705  0.907778     -0.485389   -1.35808   -2.5533  \n",
       "  0.442172    0.239211  0.805606     -0.0264728  -0.913191   1.00261 ,[1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100  …  1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100],[1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100  …  1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100,1:100],Loss[quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0)  …  quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0),quadratic(1.0)],onesparse(),Regularizer[zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg()  …  zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg(),zeroreg()],5,100x5 Array{Float64,2}:\n",
       "  1.10013    -0.183838   -1.83297     1.89964    -0.966173 \n",
       " -0.0734348  -0.487534   -0.656907   -0.731216   -0.492534 \n",
       " -0.379972   -0.0939337   0.677492   -1.40028     1.20503  \n",
       "  0.436406   -0.628345    0.0328131   0.0189489   0.952169 \n",
       " -0.617276   -1.12853     1.3127     -1.04091    -1.31298  \n",
       " -0.138607    0.601925    0.985833    0.0242555   0.968652 \n",
       " -2.40258    -0.673543    1.18788    -1.39613     0.492785 \n",
       "  0.816372   -1.16319     0.869066    0.920706   -0.47404  \n",
       " -1.10323    -1.18934     0.486239    1.01994    -1.57136  \n",
       "  0.387153   -0.183219   -0.941178    0.246549   -1.06873  \n",
       " -1.22187    -0.25579     0.477135   -1.10964     0.781284 \n",
       "  1.0347     -1.26323    -0.332701    0.154483    0.0124709\n",
       "  0.24079    -1.38004     0.2737      0.303083    0.412566 \n",
       "  ⋮                                                        \n",
       "  0.208809    2.11179     0.100045    1.40434     2.0485   \n",
       " -1.32827    -1.42463    -1.52683    -0.464357   -0.0341142\n",
       " -0.746588   -1.26496    -1.7777      1.10072     0.269523 \n",
       "  0.271841   -0.250957    0.24765     1.96433    -1.7124   \n",
       " -0.343938    1.19909     1.03624     0.465728   -1.11597  \n",
       "  1.36053    -0.636007    2.27644    -0.571678    1.2356   \n",
       " -0.0568971  -0.959228    0.768814   -0.152286    1.8198   \n",
       " -0.885416    0.368978    0.527505   -1.31022     2.896    \n",
       "  0.318107   -0.0325683   0.380378    0.802784    0.133055 \n",
       " -1.16715     0.881138    0.872498    0.213306    0.990272 \n",
       " -0.35821     0.920247    0.056257    0.104242    0.0284916\n",
       " -0.449284    2.36899     0.875609   -0.851392    0.434065 ,5x100 Array{Float64,2}:\n",
       " -0.625071   0.212897   2.65952   …  0.160482   0.848642   -1.46139 \n",
       " -1.14039   -0.252928  -0.104428     0.197839  -0.721368   -0.423369\n",
       "  0.335624  -1.62566   -0.497036     0.740155  -1.02411    -0.310835\n",
       " -0.269201  -1.91135    0.102457     1.51837    0.0885482   0.465497\n",
       " -0.972295   0.11439    1.51749      0.125566  -1.20266    -1.58396 )"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LowRankModels\n",
    "# problem dimensions\n",
    "m,n,k = 100,100,5\n",
    "# generate clustered data\n",
    "Y = randn(k,n)\n",
    "A = zeros(m,n)\n",
    "for i=1:m\n",
    "    A[i,:] = Y[mod(i,k)+1,:]\n",
    "end\n",
    "# quadratic loss\n",
    "losses = fill(quadratic(),n)\n",
    "# set regularization: x is 1-sparse, y is not regularized\n",
    "rx = onesparse()\n",
    "ry = zeroreg()\n",
    "# form GLRM\n",
    "glrm = GLRM(A,losses,rx,ry,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.8",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
